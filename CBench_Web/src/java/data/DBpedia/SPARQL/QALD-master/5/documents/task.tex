
The general task of QALD-5 is the following one: 
\begin{quote}
Given a natural language question or keywords, retrieve the correct answer(s) from a given repository containing both RDF data and free text. 
\end{quote}

The training questions described in the previous section are published so you can familiarize yourself with the dataset and the kind of questions that QALD asks. 
During test phase, from April 20 on, a set of different (but similar) questions without annotations are provided at the following location:
\begin{itemize} 
\item[] \url{http://greententacle.techfak.uni-bielefeld.de/~cunger/qald/5/qald-5_test_questions.xml} 
\end{itemize} 

You can participate working on the multilingual or the hybrid questions (or on both). 
Evaluation of performance on multilingual and hybrid questions is done separately, see \ref{evalmeasures} below.


\subsection{Registration} 

In order to participate, please register for the CLEF 2015 QA lab at 
\begin{center}
\url{http://clef2015-labs-registration.dei.unipd.it}
\end{center} 
This is not a registration for CLEF and as such is not strictly binding, 
but we will later use this to identify you during test phase.


\subsection{Submission}

Results can be submitted from April 20 to May 8 via the same online form used during training phase 
(note the drop down box that allows you to specify {\tt test} instead of {\tt training}):

\begin{itemize}
\item[] \href{http://greententacle.techfak.uni-bielefeld.de/~cunger/qald/index.php?x=evaltool&q=5}{\texttt{http://greententacle.techfak.uni-bielefeld.de/\textasciitilde cunger/qald/\\index.php?x=evaltool\&q=5}}
\end{itemize}
The only difference is that evaluation results are not displayed. 
You can upload results as often as you like (for example trying different configurations of your system); 
in this case the file with the best results will count.

All submissions are required to comply with the XML format specified in the previous section. 
For all questions, the dataset ID and question IDs are obligatory. 
Also you have to specify the answer(s) your system returned for that question. 
For questions that are out of the dataset's scope, the file should contain an empty answer set.

You are allowed to change the natural language question or keywords, for example by inserting quotes around named entities, 
by reformulating expressions your system struggles with, or even by using some controlled language format. 
If you do so, please document these changes, i.e. replace the provided question string or keywords by the input you used. 

Also, it is preferred if your submission leaves out all question strings and keywords except for the ones in the language your system worked on. 
So if you have a Spanish question answering system, please only provide the Spanish question string and/or keywords in your submission. 
Otherwise please mark the language in either the system name or configuration slot, when uploading it. 
This way we can properly honour your multilingual efforts.


\subsection{Evaluation measures} 
\label{evalmeasures}

Evaluation of performance on multilingual and hybrid questions is done separately. 

For both question types, participating systems will be evaluated with respect to precision and recall. 
Moreover, participants are encouraged to report performance, i.e. the average time their system takes to answer a query.

For each of the questions, your specified answers will be compared to the answers provided by the gold standard XML document. 
The evaluation tool computes precision, recall and F-measure for every question $q$:\footnote{In the case of out-of-scope 
questions, an empty answer set counts as precision and recall 1, while a non-empty answer set counts as 
precision and recall 0.}
\begin{align*}
\text{\em Recall}(q) &\, =\, \begin{array}{c} 
                  \text{number of correct system answers} \\
                  \hline 
                  \text{number of gold standard answers} \\ 
                  \end{array} \\
\text{\em Precision}(q) &\, =\, \begin{array}{c} 
                  \text{number of correct system answers} \\
                  \hline 
                  \text{number of system answers} \\ 
                  \end{array} \\
\text{\em F-measure}(q) &\, =\, \begin{array}{c}
                      2\times \text{\em Precision}\times \text{\em Recall} \\
                      \hline 
                      \text{\em Precision} + \text{\em Recall} \\
                      \end{array} \\
\end{align*}

\vspace*{-.7cm}

The tool then also computes the overall precision and recall taking the average mean of
all single precision and recall values, as well as the overall F-measure.

All these results are printed in a simple HTML output; additionally you get a list of 
all question that your tool failed to capture correctly.

You are allowed to submit results as often as you wish.


\subsection{Prizes}

We offer prizes for the top-scoring teams, both in the multilingual and the hybrid track:
\begin{itemize}
\item 200 EUR for the best system in the multilingual track (\texttt{hybrid="false"})
\item 100 EUR for the best system in the multilingual track (\texttt{hybrid="false"}) that worked on another language than English  
\item 200 EUR for the best system in the hybrid track (\texttt{hybrid="true"})
\end{itemize}
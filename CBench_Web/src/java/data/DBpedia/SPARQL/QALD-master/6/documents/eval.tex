
\subsection{Submission}

Results can be submitted from April 8 to April 15 via an online evaluation form:
\begin{itemize}
\item[] \href{http://greententacle.techfak.uni-bielefeld.de/~cunger/qald/index.php?x=evaltool&q=6}{\texttt{http://greententacle.techfak.uni-bielefeld.de/\textasciitilde cunger/qald/\\index.php?x=evaltool\&q=6}}
\end{itemize}
This form is available both during training and test phase
(note the drop down box that allows you to either specify {\tt training} or {\tt test})
and you can upload results as often as you like, for example trying different configurations of your system.
During test phase, the upload with the best results will count.
During training phase the evaluation tool will display the achieved results.

All submissions are required to comply with the JSON format specified in the previous section.
For all questions, the dataset ID and question IDs are obligatory.
Also you have to specify the answer(s) your system returned for that question.
The structure of the answers has to comply with the SPARQL result standard: \url{http://www.w3.org/TR/sparql11-results-json/}.
The bindings are obligatory, all other information can be provided or left out.
That is, the minimal form an answer of a \texttt{SELECT} query should have is the following:

\begin{lstlisting}
{ "results": {
     "bindings": [
          { "var": { "value": "42.0" }}
      ]
   }
}
\end{lstlisting}

The full answer could look as follows:

\begin{lstlisting}
{ "head":
   { "link": [],
     "vars": ["var"]
   },
  "results": {
     "distinct": false,
     "ordered":  true,
     "bindings": [
        { "var":
             { "type": "typed-literal",
               "datatype": "http://www.w3.org/2001/XMLSchema#decimal",
               "value": "42.0"
              }
         }
      ]
   }
}
\end{lstlisting}

For \texttt{ASK} queries, the answer structure is the following (with \texttt{head} being optional):

\begin{lstlisting}[escapechar=!]
  {
    "head": { },
    "boolean": true | false
  }
\end{lstlisting}

For questions that are out of the dataset's scope, the file should contain an empty answer set.

You are allowed to change the natural language question or keywords, for example by inserting quotes around named entities,
by reformulating expressions your system struggles with, or even by using some controlled language format.
If you do so, please document these changes, i.e. replace the provided question string or keywords by the input you used.

Also, it is preferred if your submission leaves out all question strings and keywords except for the ones your system worked on.
%So if you have a Spanish question answering system, please only provide the Spanish question string and/or keywords in your submission.
Otherwise please mark the language in either the system name or configuration slot when uploading a submission.
This way we can properly honour your multilingual efforts.


\subsection{Evaluation measures}
\label{evalmeasures}

Participating systems will be evaluated with respect to precision and recall.
Moreover, participants are encouraged to report performance, i.e. the average time their system takes to answer a query.

For each of the questions, your specified answers will be compared to the answers provided by the gold standard.
The evaluation tool computes precision, recall and F-measure for every question $q$:\footnote{In the case of out-of-scope
questions, an empty answer set counts as precision and recall 1, while a non-empty answer set counts as
precision and recall 0.}
\begin{align*}
\text{\em Recall}(q) &\, =\, \begin{array}{c}
                  \text{number of correct system answers} \\
                  \hline
                  \text{number of gold standard answers} \\
                  \end{array} \\
\text{\em Precision}(q) &\, =\, \begin{array}{c}
                  \text{number of correct system answers} \\
                  \hline
                  \text{number of system answers} \\
                  \end{array} \\
\text{\em F-measure}(q) &\, =\, \begin{array}{c}
                      2\times \text{\em Precision}\times \text{\em Recall} \\
                      \hline
                      \text{\em Precision} + \text{\em Recall} \\
                      \end{array} \\
\end{align*}

\vspace*{-.7cm}

The tool then computes both macro and micro precision, recall and F-measure, both globally and locally, i.e. for
\begin{itemize}
  \item all questions, and
  \item only those questions for which your system provides answers.
\end{itemize}
Therefore we recommend to include in your submission only questions that your system attempted to answer; otherwise the global and local scores will be the same.
